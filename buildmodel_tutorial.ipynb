{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`Learn the Basics <intro.html>`_ ||\n",
    "`Quickstart <quickstart_tutorial.html>`_ ||\n",
    "`Tensors <tensorqs_tutorial.html>`_ ||\n",
    "`Datasets & DataLoaders <data_tutorial.html>`_ ||\n",
    "`Transforms <transforms_tutorial.html>`_ ||\n",
    "**Build Model** ||\n",
    "`Autograd <autogradqs_tutorial.html>`_ ||\n",
    "`Optimization <optimization_tutorial.html>`_ ||\n",
    "`Save & Load Model <saveloadrun_tutorial.html>`_\n",
    "\n",
    "Build the Neural Network\n",
    "===================\n",
    "\n",
    "Neural networks comprise of layers/modules that perform operations on data.\n",
    "The `torch.nn <https://pytorch.org/docs/stable/nn.html>`_ namespace provides all the building blocks you need to\n",
    "build your own neural network. Every module in PyTorch subclasses the `nn.Module <https://pytorch.org/docs/stable/generated/torch.nn.Module.html>`_.\n",
    "A neural network is a module itself that consists of other modules (layers). This nested structure allows for\n",
    "building and managing complex architectures easily.\n",
    "\n",
    "In the following sections, we'll build a neural network to classify images in the FashionMNIST dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Device for Training\n",
    "-----------------------\n",
    "We want to be able to train our model on a hardware accelerator like the GPU,\n",
    "if it is available. Let's check to see if\n",
    "`torch.cuda <https://pytorch.org/docs/stable/notes/cuda.html>`_ is available, else we\n",
    "continue to use the CPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Class\n",
    "-------------------------\n",
    "We define our neural network by subclassing ``nn.Module``, and\n",
    "initialize the neural network layers in ``__init__``. Every ``nn.Module`` subclass implements\n",
    "the operations on input data in the ``forward`` method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of ``NeuralNetwork``, and move it to the ``device``, and print\n",
    "its structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the model, we pass it the input data. This executes the model's ``forward``,\n",
    "along with some `background operations <https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866>`_.\n",
    "Do not call ``model.forward()`` directly!\n",
    "\n",
    "Calling the model on the input returns a 10-dimensional tensor with raw predicted values for each class.\n",
    "We get the prediction probabilities by passing it through an instance of the ``nn.Softmax`` module.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Layers\n",
    "-------------------------\n",
    "\n",
    "Let's break down the layers in the FashionMNIST model. To illustrate it, we\n",
    "will take a sample minibatch of 3 images of size 28x28 and see what happens to it as\n",
    "we pass it through the network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Flatten\n",
    "^^^^^^^^^^^^^^^^^^^^^^\n",
    "We initialize the `nn.Flatten  <https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html>`_\n",
    "layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values (\n",
    "the minibatch dimension (at dim=0) is maintained).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Linear\n",
    "^^^^^^^^^^^^^^^^^^^^^^\n",
    "The `linear layer <https://pytorch.org/docs/stable/generated/torch.nn.Linear.html>`_\n",
    "is a module that applies a linear transformation on the input using its stored weights and biases.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.ReLU\n",
    "^^^^^^^^^^^^^^^^^^^^^^\n",
    "Non-linear activations are what create the complex mappings between the model's inputs and outputs.\n",
    "They are applied after linear transformations to introduce *nonlinearity*, helping neural networks\n",
    "learn a wide variety of phenomena.\n",
    "\n",
    "In this model, we use `nn.ReLU <https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html>`_ between our\n",
    "linear layers, but there's other activations to introduce non-linearity in your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.2146, -0.6965, -0.4252,  0.0903, -0.0466, -0.3215, -0.9861, -0.5393,\n",
      "         -0.2074,  0.5491, -0.5353,  0.0403, -0.1675, -0.4454,  0.1581,  0.0154,\n",
      "          0.2117,  0.3410,  0.0695, -0.3789],\n",
      "        [-0.2117, -0.2437, -0.0539,  0.3325, -0.3976, -0.0559, -0.7735, -0.5841,\n",
      "         -0.1599,  0.6433, -0.4822,  0.3598, -0.2377, -0.5862, -0.0065,  0.4646,\n",
      "          0.7632,  0.7807, -0.0037, -0.5790],\n",
      "        [-0.0794, -0.2082, -0.2818, -0.0447, -0.1115, -0.2147, -0.7289, -0.5859,\n",
      "         -0.2715,  0.5436, -0.6611,  0.2494, -0.1367, -0.6979,  0.2633,  0.3108,\n",
      "          0.2322,  0.5778, -0.2090, -0.2005]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.0000, 0.0000, 0.0903, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.5491, 0.0000, 0.0403, 0.0000, 0.0000, 0.1581, 0.0154, 0.2117, 0.3410,\n",
      "         0.0695, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3325, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.6433, 0.0000, 0.3598, 0.0000, 0.0000, 0.0000, 0.4646, 0.7632, 0.7807,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.5436, 0.0000, 0.2494, 0.0000, 0.0000, 0.2633, 0.3108, 0.2322, 0.5778,\n",
      "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Sequential\n",
    "^^^^^^^^^^^^^^^^^^^^^^\n",
    "`nn.Sequential <https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html>`_ is an ordered\n",
    "container of modules. The data is passed through all the modules in the same order as defined. You can use\n",
    "sequential containers to put together a quick network like ``seq_modules``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Softmax\n",
    "^^^^^^^^^^^^^^^^^^^^^^\n",
    "The last linear layer of the neural network returns `logits` - raw values in [-\\infty, \\infty] - which are passed to the\n",
    "`nn.Softmax <https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html>`_ module. The logits are scaled to values\n",
    "[0, 1] representing the model's predicted probabilities for each class. ``dim`` parameter indicates the dimension along\n",
    "which the values must sum to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Parameters\n",
    "-------------------------\n",
    "Many layers inside a neural network are *parameterized*, i.e. have associated weights\n",
    "and biases that are optimized during training. Subclassing ``nn.Module`` automatically\n",
    "tracks all fields defined inside your model object, and makes all parameters\n",
    "accessible using your model's ``parameters()`` or ``named_parameters()`` methods.\n",
    "\n",
    "In this example, we iterate over each parameter, and print its size and a preview of its values.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:  NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0172, -0.0081,  0.0291,  ...,  0.0308,  0.0281,  0.0204],\n",
      "        [-0.0203,  0.0127, -0.0007,  ..., -0.0311, -0.0163, -0.0304]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0184, -0.0197], device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0229,  0.0060, -0.0048,  ..., -0.0229,  0.0333,  0.0056],\n",
      "        [ 0.0114, -0.0430, -0.0280,  ..., -0.0351, -0.0247,  0.0387]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0406, -0.0220], device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0440,  0.0024, -0.0343,  ...,  0.0165, -0.0277, -0.0294],\n",
      "        [ 0.0035, -0.0186, -0.0101,  ...,  0.0090, -0.0308, -0.0403]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0005,  0.0129], device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model structure: \", model, \"\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0172, -0.0081,  0.0291,  ...,  0.0308,  0.0281,  0.0204],\n",
      "        [-0.0203,  0.0127, -0.0007,  ..., -0.0311, -0.0163, -0.0304],\n",
      "        [ 0.0145,  0.0224,  0.0097,  ..., -0.0293, -0.0086, -0.0043],\n",
      "        ...,\n",
      "        [ 0.0186,  0.0092, -0.0347,  ..., -0.0338, -0.0155,  0.0227],\n",
      "        [ 0.0280, -0.0194,  0.0188,  ..., -0.0253, -0.0155, -0.0190],\n",
      "        [ 0.0195, -0.0025, -0.0280,  ...,  0.0006,  0.0211, -0.0183]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 1.8419e-02, -1.9688e-02,  3.9215e-03,  8.2161e-03, -1.7048e-02,\n",
      "        -3.8919e-03, -7.7686e-03, -9.3847e-03,  2.8087e-03, -1.8687e-02,\n",
      "         3.4807e-02, -2.9393e-02,  8.9519e-03,  3.4492e-02,  3.1996e-03,\n",
      "         1.3750e-02,  6.7979e-03,  8.5280e-03, -2.6479e-02, -2.0055e-02,\n",
      "         1.3439e-02, -7.5949e-05, -3.9552e-03, -2.7133e-02, -2.1163e-03,\n",
      "         3.2017e-02,  1.7252e-02,  4.7899e-03, -2.4084e-02,  2.5570e-02,\n",
      "         4.9577e-03,  6.4330e-03, -9.7466e-03,  5.7347e-03,  2.8842e-02,\n",
      "         7.9711e-03, -2.2383e-03,  5.5959e-03,  1.1852e-02, -4.9084e-05,\n",
      "        -3.4400e-02, -2.8788e-02, -2.8636e-02, -3.3705e-02,  2.6235e-02,\n",
      "        -5.6798e-03, -2.5355e-03, -2.6649e-02, -1.1832e-02,  7.6727e-04,\n",
      "         2.9320e-02, -1.6428e-02, -2.3159e-02,  2.7317e-02,  2.1832e-02,\n",
      "         2.0963e-03,  6.0947e-03, -9.8443e-03, -2.2879e-02,  2.0242e-02,\n",
      "        -1.3814e-02,  4.8519e-03, -1.8994e-02,  2.3550e-02,  3.4514e-03,\n",
      "         2.6577e-02, -3.0682e-02, -2.1664e-02, -5.1150e-03,  6.5156e-03,\n",
      "        -2.5841e-02,  6.9116e-04,  8.4579e-03, -8.7863e-03,  2.0649e-02,\n",
      "        -7.3027e-04,  3.0261e-02,  1.6406e-02, -1.5579e-02, -1.5475e-02,\n",
      "        -1.7674e-02, -3.3285e-05, -4.6859e-04, -1.0124e-02, -2.4095e-02,\n",
      "         2.9265e-02,  2.3758e-03, -1.1348e-02, -2.9045e-02, -1.5506e-02,\n",
      "        -3.2089e-02, -7.1887e-03,  8.1337e-03, -9.9968e-03, -2.7358e-02,\n",
      "         2.6497e-02, -3.0163e-02,  4.2828e-03,  2.0384e-02,  3.1065e-02,\n",
      "        -3.3678e-02,  2.6928e-03,  1.6023e-02, -2.5484e-03,  7.9078e-05,\n",
      "         3.5504e-02, -7.4677e-03, -6.3141e-03,  3.1982e-02, -2.9303e-02,\n",
      "        -8.2043e-03,  1.8568e-02, -2.4980e-02,  1.6997e-03,  3.1564e-02,\n",
      "        -1.7600e-02,  6.9645e-03, -3.7680e-04,  1.3040e-02, -3.4425e-02,\n",
      "         5.4945e-03, -9.2603e-03, -2.6049e-02,  1.4807e-02,  2.7592e-03,\n",
      "        -9.4302e-03,  1.1559e-02,  2.2051e-02,  2.9576e-02,  3.7215e-03,\n",
      "        -2.9653e-02, -2.6249e-02, -6.5801e-03, -6.6902e-03,  3.3279e-02,\n",
      "         3.2053e-02,  1.6052e-02,  1.4154e-02, -2.8870e-02, -3.3928e-02,\n",
      "        -6.3366e-03, -1.1927e-02, -1.7640e-02,  3.3645e-02,  2.0025e-02,\n",
      "         1.6722e-02,  4.2314e-03, -1.4054e-02, -2.9432e-02, -4.1906e-03,\n",
      "        -1.2875e-02,  4.2428e-03,  1.5137e-02, -1.6549e-03,  1.5572e-02,\n",
      "         2.1214e-02, -7.1593e-03,  2.5689e-02, -1.2593e-02,  9.4921e-04,\n",
      "         8.6132e-03, -4.6837e-03, -1.3047e-02, -6.9684e-03, -2.5779e-02,\n",
      "         1.6083e-02,  1.1472e-02,  3.0661e-02, -2.0965e-02, -1.0083e-03,\n",
      "        -3.0486e-02, -3.5113e-02,  3.0282e-02,  6.0480e-03, -7.3290e-03,\n",
      "        -2.4058e-03, -1.2673e-02,  2.5559e-02, -1.6724e-02, -2.5143e-02,\n",
      "        -2.0156e-02,  5.3973e-03,  1.8779e-02,  1.2957e-02,  2.2673e-02,\n",
      "         1.8909e-02, -3.1733e-02,  2.8758e-02, -3.0265e-02,  3.3210e-02,\n",
      "        -3.2020e-02, -3.0092e-02,  3.0401e-02,  1.0438e-02, -2.6427e-02,\n",
      "        -1.6132e-02,  9.7955e-03, -2.0076e-02, -1.2802e-02,  3.4019e-02,\n",
      "        -2.9631e-02,  6.9861e-03, -1.1062e-02, -2.6948e-02,  3.3714e-02,\n",
      "         1.1225e-02,  2.3252e-02, -7.1317e-03, -3.8746e-03, -1.2127e-02,\n",
      "        -2.2475e-02,  3.3626e-03, -2.4982e-02, -1.6950e-03,  3.2374e-02,\n",
      "        -1.1725e-02,  3.2986e-02,  4.1456e-03, -1.5705e-02, -7.8367e-03,\n",
      "         9.0663e-03, -8.9235e-03,  1.0030e-02, -2.2680e-02, -1.6601e-02,\n",
      "        -2.4735e-02, -3.5199e-02,  3.5592e-02, -3.2816e-02,  3.5504e-02,\n",
      "         1.0213e-02,  1.2693e-03,  1.4140e-02, -8.7971e-03, -1.2962e-02,\n",
      "        -5.0065e-03, -2.6064e-02,  1.4121e-02,  9.3517e-03,  2.2199e-02,\n",
      "        -2.9145e-02,  3.2153e-02,  2.9386e-02, -1.9717e-02,  2.4962e-02,\n",
      "         4.3453e-03, -2.2093e-02, -3.0784e-02, -3.4503e-02, -2.2377e-02,\n",
      "         9.6840e-03, -3.1272e-02,  8.2650e-03, -1.4875e-02, -2.2466e-02,\n",
      "         2.7256e-02, -2.2462e-02, -2.4532e-02, -3.0424e-02, -1.8070e-02,\n",
      "         2.3867e-02,  1.5427e-02,  1.1404e-02, -1.5107e-02, -1.4064e-02,\n",
      "        -2.8269e-02,  2.9906e-02,  1.4629e-02, -1.9557e-02, -2.6336e-02,\n",
      "        -2.6758e-02,  1.4366e-02, -1.9923e-02,  1.5276e-02,  1.7848e-02,\n",
      "         3.0023e-02, -2.4770e-02, -5.2146e-04, -8.0580e-03, -2.5820e-02,\n",
      "         2.2057e-02,  3.1787e-02,  2.6623e-02,  1.1249e-02, -5.4095e-03,\n",
      "         2.8458e-02,  2.1402e-03,  1.6148e-02, -4.5352e-03,  2.4166e-02,\n",
      "         3.3818e-03, -2.0270e-02, -2.8124e-02, -1.3696e-02, -1.4301e-02,\n",
      "        -5.0429e-03,  3.4115e-02, -8.3912e-03,  1.4642e-02, -1.0073e-02,\n",
      "        -9.7980e-03,  1.0264e-02,  3.2437e-02, -1.0639e-02, -9.0252e-04,\n",
      "        -6.0056e-03,  8.3130e-04,  1.5791e-02, -3.1293e-02,  1.0250e-02,\n",
      "        -3.3548e-04, -1.9772e-02,  4.3788e-05,  2.4217e-02, -1.2573e-03,\n",
      "         2.5674e-02, -2.2779e-02, -3.0491e-02, -5.3656e-03, -4.8527e-03,\n",
      "        -1.6944e-02, -1.6565e-03, -5.5955e-03, -1.4062e-02, -2.4330e-02,\n",
      "        -2.6563e-02, -2.8460e-02, -1.0260e-02, -3.4519e-02, -8.9763e-03,\n",
      "        -7.1617e-03, -1.5358e-03,  2.9392e-02, -3.3862e-02,  3.2332e-02,\n",
      "        -1.5705e-02,  3.4555e-03,  2.6085e-02,  2.8927e-02,  2.8349e-02,\n",
      "         1.2819e-02, -2.2992e-02, -3.2584e-02, -1.4938e-02, -7.5424e-03,\n",
      "         1.2528e-03,  2.0472e-02, -2.0879e-02, -2.0024e-02, -1.7520e-02,\n",
      "        -1.1064e-02, -5.9215e-03,  7.2456e-03, -3.0293e-02, -2.7944e-02,\n",
      "        -3.2780e-03,  3.3389e-03,  2.1962e-02, -1.9942e-02, -2.1031e-02,\n",
      "         1.2976e-02, -1.1708e-02, -3.9237e-03, -3.9964e-03, -1.4165e-02,\n",
      "        -5.8424e-03,  1.7937e-02,  3.3853e-02,  2.3896e-02, -2.6048e-02,\n",
      "         3.3539e-02, -1.1855e-02,  2.8918e-02, -2.3046e-03, -1.5007e-02,\n",
      "         1.2494e-02, -2.1004e-02,  1.0286e-02,  3.0252e-02, -8.6878e-05,\n",
      "         1.8444e-02,  4.3051e-03, -3.3702e-02, -1.1942e-02, -8.6574e-03,\n",
      "        -3.0600e-02,  2.4589e-02,  3.1792e-02,  2.1794e-02, -7.4695e-03,\n",
      "        -2.4330e-02,  9.5307e-03, -3.3376e-02, -1.7445e-02,  2.3125e-02,\n",
      "         2.6021e-02,  3.4911e-02,  1.1727e-02, -1.7065e-02,  5.6952e-03,\n",
      "         1.2435e-02,  2.2790e-02,  2.3984e-02,  1.3563e-02, -1.9882e-02,\n",
      "        -8.4384e-04,  9.2508e-03, -3.4330e-02,  2.5117e-02,  1.1341e-02,\n",
      "         1.0107e-03,  3.2419e-02, -2.8644e-02,  3.0674e-02, -5.6807e-03,\n",
      "        -2.8143e-02,  2.3548e-02, -1.5909e-02,  1.8295e-02, -2.0574e-03,\n",
      "        -3.0318e-02,  4.8962e-03,  1.3868e-02,  4.7452e-03,  2.5598e-02,\n",
      "        -1.6244e-02,  3.1661e-02,  1.6747e-03, -7.0922e-03,  3.5233e-03,\n",
      "         1.8771e-03,  3.2139e-02,  7.6539e-03, -7.0831e-03, -2.3464e-02,\n",
      "        -1.2460e-02, -3.1259e-02,  3.3719e-02,  3.4528e-02, -1.9468e-02,\n",
      "         3.1179e-02,  2.0427e-02,  1.1633e-02,  1.9757e-02,  6.4004e-03,\n",
      "        -9.7839e-03, -2.6596e-02, -2.1813e-02,  2.0592e-02,  2.0475e-02,\n",
      "         1.5678e-02,  3.1750e-02, -1.8963e-02, -2.9606e-02,  3.4283e-02,\n",
      "         1.9691e-02,  2.1677e-02,  3.4622e-02, -1.7329e-02,  1.2115e-02,\n",
      "        -1.2710e-02,  2.9598e-02, -1.8338e-02, -3.0928e-03,  1.6140e-02,\n",
      "         1.0840e-02,  3.1370e-02, -1.0565e-02, -1.4599e-02,  6.9268e-04,\n",
      "        -2.5196e-02,  4.8259e-03,  2.8643e-03, -2.3722e-02,  2.1540e-02,\n",
      "        -1.1958e-02, -3.1437e-03,  2.3991e-02,  2.7565e-02,  4.5875e-03,\n",
      "         1.3250e-02, -1.7091e-02,  6.2336e-04,  4.4269e-04, -2.0321e-02,\n",
      "         6.6541e-03,  3.0113e-02, -2.3918e-02, -1.9330e-02,  3.2408e-02,\n",
      "         2.1744e-02, -3.4102e-02,  5.1584e-03,  3.2548e-03, -1.0072e-02,\n",
      "         2.6296e-02, -5.4502e-03,  1.2632e-02,  1.9623e-02,  3.6130e-03,\n",
      "         1.1647e-02, -3.2620e-02,  2.4070e-02, -2.8406e-02,  2.8457e-02,\n",
      "         3.1747e-02, -3.4726e-02, -3.2197e-02, -2.6317e-02, -7.7628e-03,\n",
      "         2.1852e-02,  2.3934e-02], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0229,  0.0060, -0.0048,  ..., -0.0229,  0.0333,  0.0056],\n",
      "        [ 0.0114, -0.0430, -0.0280,  ..., -0.0351, -0.0247,  0.0387],\n",
      "        [-0.0110, -0.0260,  0.0263,  ..., -0.0382, -0.0346,  0.0169],\n",
      "        ...,\n",
      "        [-0.0221, -0.0300, -0.0067,  ...,  0.0297, -0.0338, -0.0081],\n",
      "        [-0.0334,  0.0401, -0.0330,  ..., -0.0221,  0.0142,  0.0266],\n",
      "        [ 0.0317,  0.0250, -0.0408,  ..., -0.0241,  0.0157, -0.0339]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0406, -0.0220, -0.0423, -0.0221, -0.0133,  0.0015, -0.0237,  0.0031,\n",
      "         0.0146, -0.0322,  0.0238,  0.0053, -0.0425, -0.0046, -0.0108, -0.0407,\n",
      "         0.0374,  0.0017,  0.0422, -0.0259,  0.0047,  0.0206,  0.0381,  0.0210,\n",
      "        -0.0239, -0.0224, -0.0254,  0.0397, -0.0292,  0.0396,  0.0125, -0.0259,\n",
      "        -0.0149,  0.0015,  0.0230,  0.0422, -0.0244, -0.0245,  0.0253, -0.0044,\n",
      "         0.0251,  0.0321,  0.0383,  0.0146, -0.0014,  0.0006, -0.0432,  0.0173,\n",
      "         0.0240, -0.0402, -0.0148, -0.0295,  0.0114,  0.0169,  0.0037,  0.0148,\n",
      "         0.0209, -0.0196,  0.0291, -0.0342,  0.0359,  0.0399, -0.0185, -0.0215,\n",
      "         0.0205, -0.0141,  0.0421,  0.0436,  0.0108,  0.0253,  0.0264,  0.0057,\n",
      "        -0.0141, -0.0157,  0.0404,  0.0330,  0.0011, -0.0118, -0.0081,  0.0011,\n",
      "        -0.0222,  0.0272,  0.0173,  0.0216, -0.0081, -0.0193,  0.0134, -0.0406,\n",
      "         0.0317, -0.0120,  0.0208,  0.0043, -0.0101, -0.0050, -0.0404, -0.0343,\n",
      "         0.0021,  0.0189,  0.0138,  0.0004, -0.0055,  0.0321, -0.0399, -0.0026,\n",
      "        -0.0260,  0.0406,  0.0101,  0.0011, -0.0360,  0.0304, -0.0066,  0.0093,\n",
      "        -0.0046,  0.0044,  0.0325,  0.0265, -0.0226,  0.0114,  0.0108,  0.0366,\n",
      "        -0.0073, -0.0041,  0.0075, -0.0372, -0.0163,  0.0416,  0.0160,  0.0418,\n",
      "        -0.0243,  0.0405,  0.0120,  0.0291, -0.0389,  0.0189, -0.0050,  0.0359,\n",
      "         0.0130, -0.0404,  0.0230, -0.0384, -0.0124, -0.0422,  0.0097,  0.0284,\n",
      "         0.0130,  0.0316, -0.0277,  0.0367,  0.0132, -0.0408, -0.0420,  0.0232,\n",
      "        -0.0197, -0.0340, -0.0344,  0.0360,  0.0272,  0.0265, -0.0153, -0.0290,\n",
      "        -0.0016, -0.0310,  0.0117, -0.0150, -0.0129,  0.0179,  0.0383, -0.0080,\n",
      "        -0.0091, -0.0240, -0.0299, -0.0261, -0.0233, -0.0311, -0.0009, -0.0301,\n",
      "         0.0296, -0.0122, -0.0114,  0.0187,  0.0201,  0.0263,  0.0097,  0.0223,\n",
      "        -0.0412, -0.0337, -0.0311, -0.0297,  0.0136, -0.0051,  0.0394,  0.0272,\n",
      "        -0.0073,  0.0238,  0.0194, -0.0116, -0.0029, -0.0113,  0.0282,  0.0296,\n",
      "         0.0360,  0.0427,  0.0146,  0.0067, -0.0327, -0.0305, -0.0127,  0.0125,\n",
      "         0.0168, -0.0317,  0.0313, -0.0143, -0.0205,  0.0108, -0.0165, -0.0284,\n",
      "        -0.0257, -0.0349,  0.0136, -0.0311,  0.0290, -0.0233, -0.0195,  0.0045,\n",
      "         0.0088, -0.0050,  0.0311, -0.0239, -0.0145, -0.0068,  0.0395,  0.0080,\n",
      "         0.0314,  0.0192, -0.0060,  0.0022,  0.0106,  0.0253,  0.0321,  0.0361,\n",
      "        -0.0027,  0.0343, -0.0291, -0.0260,  0.0391,  0.0253,  0.0080, -0.0399,\n",
      "         0.0126, -0.0294, -0.0142, -0.0077,  0.0164,  0.0439,  0.0284,  0.0392,\n",
      "         0.0013,  0.0017, -0.0078,  0.0239, -0.0023, -0.0024, -0.0297, -0.0142,\n",
      "        -0.0376,  0.0340,  0.0223, -0.0407, -0.0031, -0.0410,  0.0359,  0.0003,\n",
      "        -0.0233, -0.0352, -0.0132,  0.0204,  0.0026, -0.0092,  0.0110, -0.0103,\n",
      "         0.0384,  0.0309, -0.0087,  0.0425, -0.0199, -0.0073, -0.0001,  0.0059,\n",
      "         0.0133,  0.0133,  0.0230, -0.0073, -0.0264, -0.0275, -0.0201,  0.0387,\n",
      "        -0.0084, -0.0342,  0.0322, -0.0313,  0.0188, -0.0336, -0.0004,  0.0220,\n",
      "         0.0333, -0.0201, -0.0104,  0.0044,  0.0410, -0.0319, -0.0324,  0.0270,\n",
      "         0.0007,  0.0246,  0.0393,  0.0011, -0.0372,  0.0405,  0.0136, -0.0103,\n",
      "         0.0419, -0.0404,  0.0051,  0.0101,  0.0261,  0.0385, -0.0168, -0.0325,\n",
      "        -0.0194,  0.0112,  0.0241,  0.0154,  0.0424,  0.0323, -0.0058, -0.0224,\n",
      "        -0.0314,  0.0165,  0.0043,  0.0143,  0.0303, -0.0010,  0.0220, -0.0018,\n",
      "         0.0310,  0.0256, -0.0352, -0.0072, -0.0404, -0.0272,  0.0426, -0.0422,\n",
      "         0.0259, -0.0403, -0.0221,  0.0367,  0.0142,  0.0022, -0.0296, -0.0185,\n",
      "        -0.0026,  0.0070,  0.0233,  0.0219,  0.0439,  0.0018, -0.0129,  0.0039,\n",
      "         0.0255, -0.0134, -0.0050,  0.0280, -0.0341,  0.0233, -0.0361, -0.0132,\n",
      "        -0.0358, -0.0280,  0.0150, -0.0367, -0.0374,  0.0396, -0.0424,  0.0398,\n",
      "        -0.0440, -0.0096, -0.0279,  0.0064, -0.0377,  0.0390, -0.0310, -0.0246,\n",
      "         0.0243,  0.0322,  0.0412, -0.0441, -0.0075,  0.0253,  0.0328, -0.0087,\n",
      "         0.0012,  0.0332, -0.0221,  0.0079,  0.0250, -0.0163, -0.0077, -0.0428,\n",
      "        -0.0296,  0.0044,  0.0129, -0.0004,  0.0011, -0.0282, -0.0376,  0.0345,\n",
      "        -0.0267,  0.0362, -0.0273, -0.0225, -0.0183, -0.0038, -0.0035, -0.0329,\n",
      "        -0.0109, -0.0049,  0.0320, -0.0226, -0.0374, -0.0224, -0.0338, -0.0421,\n",
      "         0.0212,  0.0279, -0.0297,  0.0191, -0.0312, -0.0045, -0.0074, -0.0305,\n",
      "        -0.0174,  0.0084, -0.0248,  0.0341, -0.0377,  0.0091, -0.0353, -0.0088,\n",
      "        -0.0431, -0.0342, -0.0366,  0.0046, -0.0204,  0.0317, -0.0012, -0.0317,\n",
      "         0.0186,  0.0285,  0.0287, -0.0200,  0.0108, -0.0146, -0.0277, -0.0224,\n",
      "        -0.0225,  0.0162,  0.0100, -0.0376, -0.0179,  0.0314,  0.0065,  0.0011,\n",
      "         0.0396,  0.0420,  0.0201,  0.0047, -0.0385, -0.0356, -0.0180,  0.0293,\n",
      "        -0.0040,  0.0230,  0.0216, -0.0372,  0.0085, -0.0414,  0.0020, -0.0364,\n",
      "        -0.0073, -0.0320,  0.0082, -0.0061,  0.0189, -0.0160,  0.0162,  0.0128,\n",
      "        -0.0276,  0.0093,  0.0433,  0.0369, -0.0012,  0.0375, -0.0221,  0.0282,\n",
      "         0.0202,  0.0283, -0.0242, -0.0211,  0.0188, -0.0063, -0.0170,  0.0118],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0440,  0.0024, -0.0343,  ...,  0.0165, -0.0277, -0.0294],\n",
      "        [ 0.0035, -0.0186, -0.0101,  ...,  0.0090, -0.0308, -0.0403],\n",
      "        [-0.0387,  0.0084,  0.0194,  ...,  0.0119, -0.0432, -0.0231],\n",
      "        ...,\n",
      "        [-0.0030, -0.0409, -0.0376,  ..., -0.0110,  0.0256,  0.0138],\n",
      "        [ 0.0435, -0.0227, -0.0437,  ...,  0.0201, -0.0389, -0.0089],\n",
      "        [-0.0074,  0.0244,  0.0027,  ...,  0.0142, -0.0190, -0.0241]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0005,  0.0129, -0.0359,  0.0306,  0.0147, -0.0395, -0.0144,  0.0077,\n",
      "        -0.0285, -0.0132], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(f\"Layer:  | Size: {param.size()} | Values : {param[:2]} \\n\")\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Reading\n",
    "--------------\n",
    "- `torch.nn API <https://pytorch.org/docs/stable/nn.html>`_\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
